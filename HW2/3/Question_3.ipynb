{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qIxFu4P4WfjT"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import datasets, layers, models, losses\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical , plot_model\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0WL0v4D2WgUQ"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert both X_train and X_test to grayscale"
      ],
      "metadata": {
        "id": "IqooTaGjSWXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YMz1aitTL7As"
      },
      "outputs": [],
      "source": [
        "X_train = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_train])\n",
        "X_test = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value normalization"
      ],
      "metadata": {
        "id": "o82e8LNoSZJq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bC-G93ltL-0L"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(50000,1024)\n",
        "X_test = X_test.reshape(10000,1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining input shape"
      ],
      "metadata": {
        "id": "zeECZw_OSb_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-1Uxuv0xMC3c"
      },
      "outputs": [],
      "source": [
        "X_train  = X_train/255\n",
        "X_test  = X_test/255"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA"
      ],
      "metadata": {
        "id": "5cIcvG5JSfPa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeJzGq3CH981"
      },
      "outputs": [],
      "source": [
        "def PCA(X , num_components):\n",
        "\n",
        "    X_meaned = X - np.mean(X , axis = 0)\n",
        "\n",
        "    cov_mat = np.cov(X_meaned , rowvar = False)\n",
        "     \n",
        "    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n",
        "     \n",
        "    sorted_index = np.argsort(eigen_values)[::-1]\n",
        "    sorted_eigenvalue = eigen_values[sorted_index]\n",
        "    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n",
        "     \n",
        "    eigenvector_subset = sorted_eigenvectors[:,0:num_components]\n",
        "     \n",
        "    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n",
        "     \n",
        "    return X_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXFKUheHIXAg"
      },
      "outputs": [],
      "source": [
        "merged_df = np.concatenate((X_train, X_test),axis=0)\n",
        "merged_df = PCA(merged_df, 50)\n",
        "\n",
        "X_train = merged_df[0:50000]\n",
        "X_test = merged_df[50000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e_WqWrMybENG"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label preprocessing"
      ],
      "metadata": {
        "id": "ZouY3Rl7Sq4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4uzTenXnWgo9"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "y_val = to_categorical(y_val , 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructing MLP model"
      ],
      "metadata": {
        "id": "isJ6FYfQSw_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDumTK4bWgsf"
      },
      "outputs": [],
      "source": [
        "def make_network():\n",
        "  network = Sequential()\n",
        "  network.add(Dense(256, activation = 'relu', input_shape = (50, )))\n",
        "  network.add(Dense(128, activation= 'relu'))\n",
        "  network.add(Dense(64, activation= 'relu'))\n",
        "  # network.add(Dense(32, activation= 'relu'))\n",
        "  # network.add(Dense(16, activation= 'relu'))\n",
        "  network.add(Dense(10, activation = 'softmax'))\n",
        "  network.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.SGD(learning_rate = 0.001), metrics= ['accuracy'])\n",
        "  return network\n",
        "network =make_network()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea8cLcJmWzJO"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',mode=\"min\", patience=5)\n",
        "history = network.fit(X_train, y_train,batch_size=32,epochs=200,validation_data = (X_val,y_val ), callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model evaluation"
      ],
      "metadata": {
        "id": "u_dAES_HS8Tg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTuwyv0fWzF8"
      },
      "outputs": [],
      "source": [
        "loss_plt = plt.figure(figsize = (10, 10))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend([\"Train accuracy\", \"Validation accuracy\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model evaluation"
      ],
      "metadata": {
        "id": "UTzJI4uYS9qi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anDplKsEWzCt"
      },
      "outputs": [],
      "source": [
        "loss_plt = plt.figure(figsize = (10, 10))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend([\"Train loss\", \"Validation loss\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "recall,precision,f1_score"
      ],
      "metadata": {
        "id": "XrU1x0mUTB2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUBSor9OWy_G"
      },
      "outputs": [],
      "source": [
        "real_label = np.argmax(y_test, axis = 1)\n",
        "predicted_label = np.argmax(network.predict(X_test), axis = 1)\n",
        "recall = recall_score(real_label, predicted_label, average='micro')\n",
        "precision = precision_score(real_label, predicted_label,average='macro')\n",
        "f1 = f1_score(real_label, predicted_label,average='macro')\n",
        "print(\"recall is :\" , recall )\n",
        "print(\"precision is :\" , precision )\n",
        "print(\"f1 is :\",f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss and Accuracy in test data, Training time"
      ],
      "metadata": {
        "id": "io8uRmF4TFUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPaZI9itWy7g"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = network.evaluate(X_test, y_test)\n",
        "print(\"loss in test data is: \", test_loss)\n",
        "print(\"accuracy in test data is : \", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating confusion matrix"
      ],
      "metadata": {
        "id": "BDfmitu3TF4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHmlqbFhWyxi"
      },
      "outputs": [],
      "source": [
        "real_label = np.argmax(y_test, axis = 1)\n",
        "predicted_label = np.argmax(network.predict(X_test), axis = 1)\n",
        "\n",
        "cm = confusion_matrix(real_label, predicted_label)\n",
        "normalized_cm = confusion_matrix(real_label, predicted_label, normalize = 'true')\n",
        "fig, ax = plt.subplots(nrows = 1, ncols = 2)\n",
        "fig.subplots_adjust(right = 3, top = 1.3)\n",
        "ax[0].title.set_text(\"confusion Matrix - without Normalization\")\n",
        "ConfusionMatrixDisplay(cm).plot(cmap = plt.cm.Reds, values_format = 'd', xticks_rotation = 45, ax = ax[0])\n",
        "ax[1].title.set_text(\"Confusion Matrix - Normalized\")\n",
        "ConfusionMatrixDisplay(normalized_cm).plot(cmap = plt.cm.Reds, values_format = ' .2f', xticks_rotation = 45, ax = ax[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cumulative explained variance"
      ],
      "metadata": {
        "id": "P9xCu61pTPlz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U19MbNKCsAsi"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train = X_train.reshape(50000,3072)\n",
        "X_test = X_test.reshape(10000,3072)\n",
        "df1 = pd.DataFrame(X_train)\n",
        "df3 = pd.DataFrame(X_test)\n",
        "\n",
        "frames = [df1,df3]\n",
        "result = pd.concat(frames)\n",
        "pca = PCA().fit(result)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder"
      ],
      "metadata": {
        "id": "saPX8Q7OTRGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGZ2Syc5Iu3Y"
      },
      "outputs": [],
      "source": [
        "In= Input(shape=(1024,))\n",
        "Encoder = Dense(50, activation='relu')(In)\n",
        "Decoder = Dense(10, activation='sigmoid')(Encoder)\n",
        "Autoencoder = Model(inputs = In, outputs = Decoder)\n",
        "AEncoder = Model(In, Encoder)\n",
        "Autoencoder.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = Autoencoder.fit(X_train, y_train, epochs=200, batch_size=32,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7kiJvfsjT-lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K4B5nNKmZKzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "correlation matrix"
      ],
      "metadata": {
        "id": "3WbEhumhTV6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import seaborn as sn\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/data.csv')\n",
        "\n",
        "corrMatrix = data.corr()\n",
        "sn.heatmap(corrMatrix, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eKaUWgL-qMNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature importance_LinearRegression"
      ],
      "metadata": {
        "id": "36_bmoKETlhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "data['date'] = pd.factorize(data['date'])[0] + 0.0\n",
        "data['street'] = pd.factorize(data['street'])[0] +0.0\n",
        "data['city'] = pd.factorize(data['city'])[0] + 0.0\n",
        "data['statezip'] = pd.factorize(data['statezip'])[0] + 0.0\n",
        "data['country'] = pd.factorize(data['country'])[0] + 0.0\n",
        "\n",
        "y = data.price\n",
        "x = data.drop('price' , axis =1 )\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(x,y)\n",
        "importance = model.coef_\n",
        "bars = ('date','bedrooms','bathroom','sqft_living','sqft_lot','floors','waterfront','view','condition',\n",
        "        'sqft_above','sqft_basement','yr_built','yr_renovated','street','city','statezip','country')\n",
        "y_pos = np.arange(len(bars))\n",
        "plt.figure(figsize=(30,10))\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.xticks(y_pos, bars)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewJW9mq6tiYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature importance_DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "irx2pI1iTtBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(x,y)\n",
        "importance = model.feature_importances_\n",
        "bars = ('date','bedrooms','bathroom','sqft_living','sqft_lot','floors','waterfront','view','condition',\n",
        "        'sqft_above','sqft_basement','yr_built','yr_renovated','street','city','statezip','country')\n",
        "y_pos = np.arange(len(bars))\n",
        "plt.figure(figsize=(30,10))\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.xticks(y_pos, bars)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HIF-Avwd-sPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Question-3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}